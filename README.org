#+TITLE: Behaviour-based Android Malware Detection
#+AUTHOR: Zhitao Gong
#+DESCRIPTION: Behaviour-based classifiers to detect android malware.
#+KEYWORDS: malware detection, classification, android
#+SETUPFILE: ~/Dropbox/emacs/setupfile.org

#+BEGIN_abstract
We investigate malware detection on Android platform based on
application behaviours, e.g., ram usage.  Besides SVM based
classifiers and logistic regression, we also investigate neural
network based solutions, e.g., feed-forward (cascaded) network,
auto-encoder, etc.
#+END_abstract

#+TOC: headlines 2

* Introduction

  We have 100 application samples, half of them are tagged malicious,
  the other half benign.  For each application object we have 1000
  data samples, collected at an interval of 1 millisecond for a
  duration of 1 second.  Totally we have 32 features, which are the
  /behaviours/ of the application, e.g., ram usage, cache usage, etc.

* Data Preprocessing

  To prepare for algorithm development, we first run the data through
  some cleanup routines.

** Feature Cleanup

   Out of 32 features, 11 of them have nearly zero variance[fn:1], i.e.,
   less then \(1e^{-6}\).  These features are removed since they
   do not provide support for our classification.

   Figure [[fig:tsne_full_feature]] shows the distribution of all our data
   samples without feature selection.

   #+NAME: fig:tsne_full_feature
   #+CAPTION: TSEN with all features
   [[file:./img/tsne_all_features.svg]]

   Different parameters of t-SNE produce slightly different
   distribution results.  However, of all the results from different
   parameter combinations, I did not find any clear clusters.  Most of
   the time these two categories are mingled together.  Possibile
   reasons for this are

   1. <<lst:p1>>Sample size is too limited.
   2. Feature set is too large, relative to the sample size.
   3. <<lst:p2>>Features do not provide enough support for classification.


   Problem [[lst:p1]] and problem [[lst:p2]] are out of my control.  we may
   try reducing the feature set to see if any clear patterns can be
   spotted.

** Data Normalization

   Features are all numerical data, in different ranges.  So we
   normalize them into the similar magnitude.

   Each data object is measured for 1 second at an interval of 1
   millisecond.  So each data object actually contains "time series"
   data.  However, quick analysis of the "time series" data shows that
   all the features have almost zero variance for each data sample.
   An intuitive explanation for this is that the sample rate is too
   high.  Instead of a "time series" for each data object, we have
   only one sample per data object.  In the following writing, we use
   data sample to denote the data for each tagged software.

** Randomized Test

   After the previous two steps, we have the usual classification
   problem, for which there are totally 100 data samples, half of them
   are tagged 0 (benign software) and the other half tagged 1
   (maliciout software).  We use 50% as training set.  For each test,
   we randomly draw 25 samples from each group and test them on our
   classifiers.

* Experiment

  We summarize the performance for each classifier.

** SVM

   Table [[tab:svm_all_feature_result]] summarizes the test result for
   classifier based on Support Vector Machine (SVM).  This is the
   averaged result after 100 test runs.

   #+NAME: src:svm_all_feature_result
   #+BEGIN_SRC python :exports results :results value table append
import numpy as np
res = np.genfromtxt('data/test_svm_result.csv',
                    delimiter=',', skip_header=1)
res = np.mean(res, 0)
ret = [['Pred Benware', '{:.4f}'.format(res[0]), '{:.4f}'.format(res[2])],
       ['Pred Malware', '{:.4f}'.format(res[1]), '{:.4f}'.format(res[3])]]
return ret
   #+END_SRC

   #+NAME: tab:svm_all_feature_result
   #+CAPTION: SVC with full feature set result
   #+RESULTS: src:svm_all_feature_result
   |   | Benware | Malware |
   |---+---------+---------|

   The interpretation for Table [[tab:svm_all_feature_result]] is as
   follows.

   1. Notice that half of the test set are tagged malware, the other
      half benign.
   2. For both categories, the accuracy is around 80%, i.e., 20% is
      wrongly classified.

* Footnotes
[fn:1] Actually they all have zero variance.
