#+TITLE: Android Malware Detection
#+AUTHOR: Zhitao Gong
#+DESCRIPTION: Behaviour-based classifiers to detect android malware.
#+KEYWORDS: malware detection, classification, android

#+HTML_HEAD: <link rel="stylesheet" href="normalize.css" type="text/css" />
#+HTML_HEAD: <link rel="stylesheet" href="https://gongzhitaao.github.io/orgcss/org.css" type="text/css" />

* Introduction

  We have 100 application samples, half of them are tagged malevolent,
  the other half benign.  For each application object we have 1000
  data samples, collected at an interval of 1 millisecond for duration
  of 1 second.  Totally we have 32 features, which are the /behaviors/
  of the application, e.g., ram usage, cache usage, etc.

* Data Preprocessing

  To prepare for algorithm development, we first run the data through
  some cleanup routines.

** Feature Cleanup

   Out of 32 features, 11 of them have standard deviation of nearly
   zero, i.e., less then \(1e^{-6}\)[fn:1], which means these features
   do not provide support for our classification.  As a result, they
   are removed.

   We have 50 data objects for each category, half of which are used
   as training set and the other half as test set.  It seems that we
   may not need to reduce the feature set.  However, we will conduct
   further experiments.

** Data Normalization

   Features are all numerical data, in different ranges.  So it makes
   sense to normalize them into the same magnitude.

   Each data object is measured for 1 second at an interval of 1
   millisecond.  So each data object actually contains "time series"
   data.  However, quick analysis of the "time series" data shows that
   all the features have identical values for each data sample.  An
   intuitive explanation for this is the sample rate is too high.
   Instead of a "time series" for each data object, we have only one
   sample per data object.  In the following writing, we use data
   sample to denote the data for each tagged software.

** Randomized Test

   After the previous two steps, we have the usual classification
   problem, for which there are totally 100 data samples, half of them
   are tagged 1 and the other half tagged 2.  We use 50 of them as
   training samples.  For each test, we randomly draw 25 samples from
   each group and test them on our classifiers.

* Experiment

  We summarize the performance for each classifier.

** SVM

   Figure [[tab:svm_result]] summarize the test result for classifier
   based on Support Vector Machine (SVM).  This is the averaged result
   after 100 test runs.

   #+NAME: tab:svm_result
   #+CAPTION: 100 SVC Test Run Result
   |              | Benware | Malware |
   |--------------+---------+---------|
   | Pred Benware | 0.4628  | 0.0372  |
   | Pred Malware | 0.0492  | 0.4508  |

   Totally, the accuracy is around 91%.

* Footnotes

[fn:1] Actually they have standard deviation of exactly zero, which
means they keep the same values for all data samples respectively.

#  LocalWords:  Malware Zhitao malware fn SVM svm SVC Benware Pred
